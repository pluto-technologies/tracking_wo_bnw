{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "from os import path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sacred import Experiment\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm.notebook import tqdm\n",
    "from tracktor.config import get_output_dir\n",
    "from tracktor.datasets.factory import Datasets\n",
    "from tracktor.datasets.custom_wrapper import CustomSequence\n",
    "from tracktor.frcnn_fpn import FRCNN_FPN\n",
    "from tracktor.oracle_tracker import OracleTracker\n",
    "from tracktor.reid.resnet import ReIDNetwork_resnet50\n",
    "from tracktor.tracker import Tracker\n",
    "from tracktor.utils import (evaluate_mot_accums, get_mot_accum,\n",
    "                            interpolate_tracks, plot_sequence)\n",
    "from torchvision.ops.boxes import clip_boxes_to_image, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    #img = cv2.resize(img, (512, 512))\n",
    "    return img\n",
    "\n",
    "def hist_eq(img):\n",
    "    hist,bins = np.histogram(img.flatten(),256,[0,256])\n",
    "    cdf = hist.cumsum()\n",
    "    cdf_normalized = cdf * hist.max()/ cdf.max()\n",
    "    cdf_m = np.ma.masked_equal(cdf,0)\n",
    "    cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\n",
    "    cdf = np.ma.filled(cdf_m,0).astype('uint8')\n",
    "    return cdf[img]\n",
    "\n",
    "def print_img(img):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(img, cmap='Greys_r')\n",
    "    \n",
    "def draw_bounding(img, x1, x2, y1, y2, label):\n",
    "    # draw a rectangle on the image\n",
    "    h, w = img.shape[:2]\n",
    "    cv2.putText(img, label , (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,0,255), 2)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_name = 'b√¶kvej_faxe'\n",
    "img_dir = \"../../data/\" + seq_name\n",
    "file_names = sorted(os.listdir(img_dir), key = lambda x: x[:4])\n",
    "file_path = [img_dir  + \"/\" + name for name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoU((1,1,4,4),(3,2,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee41d9a942cb4b3699ee80707e6919d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2160), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj_detect_model = \"../../output/custom/model_epoch_30.model\"\n",
    "obj_detect = FRCNN_FPN(num_classes=31)\n",
    "obj_detect.load_state_dict(torch.load(obj_detect_model, map_location=lambda storage, loc: storage))\n",
    "obj_detect.eval()\n",
    "obj_detect.cuda();\n",
    "transforms = ToTensor()\n",
    "\n",
    "track_count = {}\n",
    "track_score = {}\n",
    "track_label = {}\n",
    "available = {}\n",
    "curr_track_id = 0\n",
    "all_boxes = [{}]\n",
    "\n",
    "for i in tqdm(range(len(file_path))):\n",
    "    img = Image.open(file_path[i]).convert(\"RGB\")\n",
    "    img_tensor = transforms(img).unsqueeze(0)\n",
    "    obj_detect.load_image(img_tensor)\n",
    "    \n",
    "    prev_frame = all_boxes[-1]\n",
    "    \n",
    "    stage = {}\n",
    "    remove = set()\n",
    "    \n",
    "    this_frame = {}\n",
    "    \n",
    "    if len(prev_frame) != 0:\n",
    "        box_list = [prev_frame[track_id].unsqueeze(0) for track_id in prev_frame]\n",
    "        label_list = [track_label[track_id].unsqueeze(0) for track_id in prev_frame]\n",
    "        boxes = torch.cat(box_list, 0)\n",
    "        labels = torch.cat(label_list, 0)\n",
    "        \n",
    "        new_boxes, new_scores = obj_detect.predict_boxes(boxes, labels, -1)\n",
    "        new_boxes = clip_boxes_to_image(new_boxes, img_tensor.shape[-2:])\n",
    "        \n",
    "        track_num = 0\n",
    "        for track_id in prev_frame:\n",
    "            stage[track_id] = new_boxes[track_num]\n",
    "            track_score[track_id] = new_scores[track_num]\n",
    "            if new_scores[track_num] < 0.01:\n",
    "                remove.add(track_id)\n",
    "            track_num += 1\n",
    "                \n",
    "                \n",
    "    det_boxes, det_labels, det_scores = obj_detect.detect(img_tensor, -1)\n",
    "    det_boxes = clip_boxes_to_image(det_boxes, img_tensor.shape[-2:])\n",
    "    keep = nms(det_boxes, det_scores, 0.1).cuda()\n",
    "    taken = set()\n",
    "    \n",
    "    # back up the tracks\n",
    "    for track_id in stage:\n",
    "        track_box = stage[track_id]\n",
    "        consider = []\n",
    "        for det_id in keep:\n",
    "            det_box, det_label, det_score = det_boxes[det_id], det_labels[det_id], det_scores[det_id]\n",
    "            if (det_score > 0.4 and \n",
    "                det_score > track_score[track_id] and\n",
    "                IoU(det_box, track_box) > 0.2 and\n",
    "                det_label == track_label[track_id]):\n",
    "                    \n",
    "                taken.add(det_id)\n",
    "                consider.append(det_id)\n",
    "                    \n",
    "        if len(consider) != 0:\n",
    "            high_ind = max(consider,\n",
    "                           key = lambda x: det_scores[x])#IoU(track_box, det_boxes[x]))\n",
    "            if track_id in remove:\n",
    "                remove.remove(track_id)\n",
    "            stage[track_id] = det_boxes[high_ind]\n",
    "            track_score[track_id] = det_scores[high_ind]\n",
    "        if len(taken) == len(keep):\n",
    "            break\n",
    "                \n",
    "    # initialize new tracks\n",
    "    for det_id in keep:\n",
    "        if det_id not in taken:\n",
    "            det_box, det_label, det_score = det_boxes[det_id], det_labels[det_id], det_scores[det_id]\n",
    "            toAdd = True\n",
    "            for track_id in stage:\n",
    "                track_box = stage[track_id]\n",
    "                if(IoU(track_box, det_box) > 0.1):\n",
    "                    toAdd = False                   \n",
    "            if det_score > 0.4 and toAdd:\n",
    "                new_track = curr_track_id\n",
    "                curr_track_id += 1\n",
    "\n",
    "                track_label[new_track] = det_label\n",
    "                stage[new_track] = det_box\n",
    "                track_score[new_track] = det_score\n",
    "                track_count[new_track] = 0\n",
    "                \n",
    "    # Transfer from stage to the array\n",
    "    for track_id in stage:\n",
    "        if track_id not in remove:\n",
    "            this_frame[track_id] = stage[track_id]\n",
    "            track_count[track_id] += 1\n",
    "            \n",
    "    all_boxes.append(this_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# This is to delete the tracks with less than 10 frames\n",
    "re_id = 0\n",
    "renumeration = {}\n",
    "for frame in all_boxes:\n",
    "    for track_id in frame:\n",
    "        if track_count[track_id] > 10 and track_id not in renumeration:\n",
    "            renumeration[track_id] = re_id\n",
    "            re_id += 1\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e72ab945ff4129b0f48921909926a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2160), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Record the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "writer = cv2.VideoWriter(\"tracking.mp4\", fourcc, 30, (1024, 1024), True)\n",
    "\n",
    "i = 0\n",
    "for frame in tqdm(all_boxes[1:]):\n",
    "    cv_img = get_img(file_path[i])\n",
    "    i+= 1\n",
    "    output_img = cv_img.copy()\n",
    "    for track_id in frame:\n",
    "        if track_id in renumeration:\n",
    "            box = frame[track_id]\n",
    "            label = frame[track_id]\n",
    "            score = frame[track_id]\n",
    "            score = score.cpu().numpy()\n",
    "            label = label.cpu().numpy()\n",
    "\n",
    "            image_text = str(track_id) + \" \" + str(score) \n",
    "            draw_bounding(output_img, box[0], box[2], box[1], box[3], str(track_id))\n",
    "    \n",
    "    writer.write(output_img)     \n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
